{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0301524c-c75f-4769-9421-b31c4fc6f0e0",
   "metadata": {},
   "source": [
    "# Example: Develop a Multiclass Artificial Neural Network Image Classifier\n",
    "This example will familiarize students with constructing, training, and testing a simple [Feed-Forward Artificial Neural Network](https://en.wikipedia.org/wiki/Feedforward_neural_network) that will classify images of handwritten numbers between `0,...,9` taken from the [Modified National Institute of Standards and Technology (MNIST) database](https://en.wikipedia.org/wiki/MNIST_database). Each digit between `0` and `9` has approximately 5000 example images, each of which is a `28`$\\times$`28` grayscale image; thus, each image has `784` pixels.  \n",
    "\n",
    "* In this example, we'll use the [Flux.jl package](https://github.com/FluxML/Flux.jl) to build, train, and test our image classifier. However, there are two excellent libraries for ANNs in Python (sort of), namely the [PyTorch library](https://pytorch.org/) from the [AI group at META](https://ai.meta.com/meta-ai/) and the [TensorFlow library](https://www.tensorflow.org/) developed by [Google](https://research.google/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e0043b-6b9a-4851-b1ff-ce4fef45e091",
   "metadata": {},
   "source": [
    "## Setup\n",
    "This example requires several external libraries and a function to compute the outer product. Let's download and install these packages and call our `Include.jl` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cc952ea-d4e4-449b-aec7-ddc44d741972",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Desktop/julia_work/CHEME-4800-5800-Examples-AY-2024/week-15/L15c/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Desktop/julia_work/CHEME-4800-5800-Examples-AY-2024/week-15/L15c/Manifest.toml`\n"
     ]
    }
   ],
   "source": [
    "include(\"Include.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f403b43-7d09-4058-a96c-aae3bbfca8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_extension(file::String) = file[findlast(==('.'), file)+1:end];"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc914a79-bf7f-4889-b1d1-54003da268d8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Prerequisites\n",
    "Before training and testing the `ANN,` we need to construct two datasets. First, we need to build a `training dataset` of images that we will use to estimate the model parameters. We'll save training data in the `training_image_dataset` variable. Next, we'll construct a `test dataset,` which we'll use to see how well our `ANN` predicts data it has never seen. We'll save this data in the `testing_image_dataset` variable.\n",
    "* Both the `training_image_dataset` and `testing_image_dataset` will be of type `Vector{Tuple{Vector{Float32}, OneHotVector{UInt32}}}` where the first element is the input data `x.` The second element is the `label,` i.e., whether the image corresponds to `0,....,9`.\n",
    "* The `Vector{Tuple{Vector{Float32}, OneHotVector{UInt32}}}` type has a couple of weird features. First, notice that the floating point is `Float32`, not the default `Float64`. Next, the labels are [One Hot ecoded](https://en.wikipedia.org/wiki/One-hot). Finally, the input data `x` is a Vector, not a Matrix (even though the original image is a matrix of `Gray` values)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c583d83-6353-450f-9a6f-2ec5b44185c8",
   "metadata": {},
   "source": [
    "### Select a set of `training` images, and build the `training_image_dataset`\n",
    "`Unhide` the code blocks below to see how we construct and populate the `training_image_dataset` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f7edf7d-4bf1-468b-9c15-dc847a3c4f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_training_examples = 3000;\n",
    "number_digit_array = range(0,length=10,step=1) |> collect;\n",
    "training_image_dictionary = Dict{Int64, Array{Gray{N0f8},3}}();\n",
    "for i ∈ number_digit_array\n",
    "    \n",
    "    # create a set for this digit -\n",
    "    image_digit_array = Array{Gray{N0f8},3}(undef, 28, 28, number_of_training_examples);\n",
    "    files = readdir(joinpath(_PATH_TO_IMAGES,\"$(i)\")); \n",
    "    imagecount = 1;\n",
    "    for fileindex ∈ 1:number_of_training_examples\n",
    "        filename = files[fileindex];\n",
    "        ext = file_extension(filename)\n",
    "        if (ext == \"jpg\")\n",
    "            image_digit_array[:,:,fileindex] = joinpath(_PATH_TO_IMAGES, \"$(i)\", filename) |> x-> FileIO.load(x);\n",
    "            imagecount += 1\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # capture -\n",
    "    training_image_dictionary[i] = image_digit_array\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1554fa2-cfeb-4776-a186-be578f27fffd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "training_image_dataset = Vector{Tuple{Vector{Float32}, OneHotVector{UInt32}}}()\n",
    "for i ∈ number_digit_array\n",
    "    Y = onehot(i, number_digit_array);\n",
    "    X = training_image_dictionary[i];\n",
    "    \n",
    "    for t ∈ 1:number_of_training_examples\n",
    "        D = Array{Float32,1}(undef, 28*28);\n",
    "        linearindex = 1;\n",
    "        for row ∈ 1:28\n",
    "            for col ∈ 1:28\n",
    "                D[linearindex] = X[row,col,t] |> x-> convert(Float32,x);\n",
    "                linearindex+=1;\n",
    "            end\n",
    "        end\n",
    "\n",
    "        training_tuple = (D,Y);\n",
    "        push!(training_image_dataset,training_tuple);\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f28bf4b-8671-4975-bf37-95d3b0d8a072",
   "metadata": {},
   "source": [
    "### Select a set of `test` images, and build the `testing_image_dataset`\n",
    "`Unhide` the code blocks below to see how we construct and populate the `testing_image_dataset` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62ad3577-cdd3-491b-8b14-636e0c913c8a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "number_of_test_examples = 500;\n",
    "testing_image_dictionary = Dict{Int64, Array{Gray{N0f8},3}}();\n",
    "for i ∈ number_digit_array\n",
    "    \n",
    "    # create a set for this digit -\n",
    "    image_digit_array = Array{Gray{N0f8},3}(undef, 28, 28, number_of_test_examples);\n",
    "    files = readdir(joinpath(_PATH_TO_IMAGES,\"$(i)\")); \n",
    "    imagecount = 1;\n",
    "    for fileindex ∈ (number_of_training_examples + 1):(number_of_training_examples+number_of_test_examples)\n",
    "        filename = files[fileindex];\n",
    "        ext = file_extension(filename)\n",
    "        if (ext == \"jpg\")\n",
    "            image_digit_array[:,:,imagecount] = joinpath(_PATH_TO_IMAGES, \"$(i)\", filename) |> x-> FileIO.load(x);\n",
    "            imagecount += 1\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # capture -\n",
    "    testing_image_dictionary[i] = image_digit_array\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad1c5708-f557-4625-899e-24bb8605f641",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "testing_image_dataset = Vector{Tuple{Vector{Float32}, OneHotVector{UInt32}}}()\n",
    "for i ∈ number_digit_array\n",
    "    Y = onehot(i, number_digit_array);\n",
    "    X = testing_image_dictionary[i];\n",
    "    \n",
    "    for t ∈ 1:number_of_test_examples\n",
    "        D = Array{Float32,1}(undef, 28*28);\n",
    "        linearindex = 1;\n",
    "        for row ∈ 1:28\n",
    "            for col ∈ 1:28\n",
    "                D[linearindex] = X[row,col,t] |> x-> convert(Float32,x);\n",
    "                linearindex+=1;\n",
    "            end\n",
    "        end\n",
    "\n",
    "        testing_tuple = (D,Y);\n",
    "        push!(testing_image_dataset, testing_tuple);\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148fb784-2674-4675-90a8-f9fda79525ed",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c6bae7b-1671-4bfa-8f84-89ca84152187",
   "metadata": {},
   "outputs": [],
   "source": [
    "should_we_train = false;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18499f7b-5bd3-4a58-939c-633f9831df09",
   "metadata": {},
   "source": [
    "## Setup the model structure and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f7f536e-0bc5-43c1-8258-7ab7e13ead59",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_input_states = length(training_image_dataset[1][1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c6d8b2-fde1-4a36-8a32-1f4d59ba39c7",
   "metadata": {},
   "source": [
    "Let's build an empty model with default parameter values but a fixed structure, i.e., the number and dimension of the layers, and the activation functions for each layer are specified when we build the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bbf74b6b-8aea-494b-b805-159d061acf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "Flux.@layer MyFluxNeuralNetworkModel;\n",
    "MyModel() = MyFluxNeuralNetworkModel(\n",
    "    Chain(\n",
    "        Dense(number_of_input_states, 512, relu),  \n",
    "        Dense(512, 10, relu),\n",
    "        NNlib.softmax)\n",
    ");\n",
    "model = MyModel().chain;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431f73bc-28ba-4a9e-af0b-ebc0d42392b0",
   "metadata": {},
   "source": [
    "Next, specify the `loss` function that we will minimize to to esimate the model parameters. In this cross we choose a loss function that is appropriate for a multiclass classification problem, namely a [cross entropy loss function](https://en.wikipedia.org/wiki/Cross-entropy):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e432c99f-b8a5-4159-b1f3-5b77667c72a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup a loss function -\n",
    "loss(ŷ, y) = Flux.Losses.logitcrossentropy(ŷ, y; agg = mean);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef59a113-f473-4c02-a409-da18ccb088e7",
   "metadata": {},
   "source": [
    "Then, let's specify which [Gradient descent method]() we will use to search parameter space and estimate the set of parameters that minimizes the `loss` function specified above. \n",
    "* In this case, we'll use [Gradient descent with momentum](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Momentum) where the `λ` parameter denotes the `learning rate` and `β` denotes the momentum parameter. We save information about the optimizer in the `opt_state` variable, which will eventually get passed to the training method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "368796dc-7bd9-4c3a-8d88-4092820393f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "λ = 0.01; # learning rate\n",
    "β = 0.90; # momentum parameter\n",
    "opt_state = Flux.setup(Momentum(λ, β), model);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a57be96-4f45-4536-8e86-1d57a2caa24f",
   "metadata": {},
   "source": [
    "We are now ready to train the model. If the `should_we_train` flag is true, then we use the [Gradient descent with momentum](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Momentum) to minimize a [cross entropy loss function](https://en.wikipedia.org/wiki/Cross-entropy). \n",
    "* Because of the error landscape's non-convex nature, we have to start from many different locations. We do `number_of_epochs` passes through the data, i.e., a forward pass for prediction and a backpropagation step for parameter updates.\n",
    "* Training takes a long time. Thus, for each complete pass through the data, i.e., for each `epoch,` we save a `tmp` file holding the network state... just in case of `BOOOOOOOOM.`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2c0e9b70-f312-4e3e-8693-c8d3350a4804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training example: 1111 has loss = 2.4611495 in epoch 1\n",
      "Training example: 1660 has loss = 2.4566433 in epoch 2\n",
      "Training example: 276 has loss = 2.4607952 in epoch 3\n",
      "Training example: 727 has loss = 2.4611502 in epoch 4\n",
      "Training example: 825 has loss = 2.4611273 in epoch 5\n",
      "Training example: 212 has loss = 2.346879 in epoch 6\n",
      "Training example: 596 has loss = 1.9799261 in epoch 7\n",
      "Training example: 2133 has loss = 2.3984303 in epoch 8\n",
      "Training example: 28 has loss = 2.001026 in epoch 9\n",
      "Training example: 232 has loss = 1.4672277 in epoch 10\n",
      "Training example: 2690 has loss = 2.4611502 in epoch 11\n",
      "Training example: 678 has loss = 2.4611502 in epoch 12\n",
      "Training example: 1259 has loss = 2.4173224 in epoch 13\n",
      "Training example: 2796 has loss = 2.4611304 in epoch 14\n",
      "Training example: 756 has loss = 2.4611502 in epoch 15\n",
      "Training example: 290 has loss = 2.4611502 in epoch 16\n",
      "Training example: 367 has loss = 2.4611502 in epoch 17\n",
      "Training example: 2525 has loss = 2.4611502 in epoch 18\n",
      "Training example: 359 has loss = 1.46115 in epoch 19\n",
      "Training example: 2083 has loss = 1.461426 in epoch 20\n",
      "Training example: 134 has loss = 2.4611502 in epoch 21\n",
      "Training example: 2582 has loss = 1.46115 in epoch 22\n",
      "Training example: 1599 has loss = 1.4611742 in epoch 23\n",
      "Training example: 1486 has loss = 2.4610863 in epoch 24\n",
      "Training example: 1313 has loss = 1.46115 in epoch 25\n",
      "Training example: 2269 has loss = 2.4611502 in epoch 26\n",
      "Training example: 2831 has loss = 2.4611382 in epoch 27\n",
      "Training example: 1890 has loss = 2.4611502 in epoch 28\n",
      "Training example: 2332 has loss = 2.4611502 in epoch 29\n",
      "Training example: 1581 has loss = 2.4598205 in epoch 30\n",
      "Training example: 381 has loss = 2.4611502 in epoch 31\n",
      "Training example: 738 has loss = 2.431513 in epoch 32\n",
      "Training example: 514 has loss = 2.4611502 in epoch 33\n",
      "Training example: 1594 has loss = 2.4611502 in epoch 34\n",
      "Training example: 723 has loss = 2.4611502 in epoch 35\n",
      "Training example: 572 has loss = 1.5130715 in epoch 36\n",
      "Training example: 382 has loss = 2.4567487 in epoch 37\n",
      "Training example: 851 has loss = 2.4611502 in epoch 38\n",
      "Training example: 1119 has loss = 1.46115 in epoch 39\n",
      "Training example: 2036 has loss = 1.46115 in epoch 40\n",
      "Training example: 2645 has loss = 2.4593487 in epoch 41\n",
      "Training example: 111 has loss = 2.4611502 in epoch 42\n",
      "Training example: 1590 has loss = 1.46115 in epoch 43\n",
      "Training example: 2128 has loss = 2.4611502 in epoch 44\n",
      "Training example: 916 has loss = 1.46115 in epoch 45\n",
      "Training example: 2345 has loss = 1.46115 in epoch 46\n",
      "Training example: 1177 has loss = 2.4611502 in epoch 47\n",
      "Training example: 1966 has loss = 1.46115 in epoch 48\n",
      "Training example: 2132 has loss = 1.46115 in epoch 49\n",
      "Training example: 2591 has loss = 1.46115 in epoch 50\n",
      "Training example: 2355 has loss = 1.46115 in epoch 51\n",
      "Training example: 2426 has loss = 2.4611502 in epoch 52\n",
      "Training example: 366 has loss = 1.46115 in epoch 53\n",
      "Training example: 1223 has loss = 1.461151 in epoch 54\n",
      "Training example: 1164 has loss = 1.46115 in epoch 55\n",
      "Training example: 2379 has loss = 1.46115 in epoch 56\n",
      "Training example: 1473 has loss = 1.46115 in epoch 57\n",
      "Training example: 968 has loss = 2.4611502 in epoch 58\n",
      "Training example: 892 has loss = 1.46115 in epoch 59\n",
      "Training example: 73 has loss = 1.46115 in epoch 60\n",
      "Training example: 2104 has loss = 2.4611502 in epoch 61\n",
      "Training example: 1353 has loss = 1.46115 in epoch 62\n",
      "Training example: 1662 has loss = 1.46115 in epoch 63\n",
      "Training example: 2933 has loss = 1.46115 in epoch 64\n",
      "Training example: 1178 has loss = 2.4611502 in epoch 65\n",
      "Training example: 719 has loss = 2.4611502 in epoch 66\n",
      "Training example: 830 has loss = 1.8344902 in epoch 67\n",
      "Training example: 583 has loss = 1.46115 in epoch 68\n",
      "Training example: 1674 has loss = 1.46115 in epoch 69\n",
      "Training example: 516 has loss = 1.46115 in epoch 70\n",
      "Training example: 1039 has loss = 1.46115 in epoch 71\n",
      "Training example: 774 has loss = 1.46115 in epoch 72\n",
      "Training example: 1408 has loss = 1.46115 in epoch 73\n",
      "Training example: 1281 has loss = 2.4611502 in epoch 74\n",
      "Training example: 989 has loss = 1.46115 in epoch 75\n",
      "Training example: 2388 has loss = 1.46115 in epoch 76\n",
      "Training example: 2023 has loss = 2.4611502 in epoch 77\n",
      "Training example: 440 has loss = 2.4611502 in epoch 78\n",
      "Training example: 2668 has loss = 1.46115 in epoch 79\n",
      "Training example: 1172 has loss = 1.46115 in epoch 80\n",
      "Training example: 2693 has loss = 1.46115 in epoch 81\n",
      "Training example: 170 has loss = 1.46115 in epoch 82\n",
      "Training example: 2172 has loss = 1.46115 in epoch 83\n",
      "Training example: 2957 has loss = 1.46115 in epoch 84\n",
      "Training example: 1580 has loss = 1.46115 in epoch 85\n",
      "Training example: 1512 has loss = 2.4611502 in epoch 86\n",
      "Training example: 2212 has loss = 1.46115 in epoch 87\n",
      "Training example: 2147 has loss = 1.46115 in epoch 88\n",
      "Training example: 2852 has loss = 1.46115 in epoch 89\n",
      "Training example: 282 has loss = 1.46115 in epoch 90\n",
      "Training example: 589 has loss = 1.46115 in epoch 91\n",
      "Training example: 2782 has loss = 1.46115 in epoch 92\n",
      "Training example: 725 has loss = 2.4234943 in epoch 93\n",
      "Training example: 2559 has loss = 1.46115 in epoch 94\n",
      "Training example: 1827 has loss = 1.46115 in epoch 95\n",
      "Training example: 1502 has loss = 1.46115 in epoch 96\n",
      "Training example: 128 has loss = 1.46115 in epoch 97\n",
      "Training example: 1272 has loss = 1.46115 in epoch 98\n",
      "Training example: 1730 has loss = 1.46115 in epoch 99\n",
      "Training example: 1342 has loss = 1.46115 in epoch 100\n",
      "Training example: 1271 has loss = 1.46115 in epoch 101\n",
      "Training example: 2819 has loss = 1.46115 in epoch 102\n",
      "Training example: 244 has loss = 2.4611502 in epoch 103\n",
      "Training example: 2682 has loss = 2.4611502 in epoch 104\n",
      "Training example: 2957 has loss = 1.46115 in epoch 105\n",
      "Training example: 916 has loss = 1.46115 in epoch 106\n",
      "Training example: 738 has loss = 2.4611502 in epoch 107\n",
      "Training example: 462 has loss = 1.46115 in epoch 108\n",
      "Training example: 2946 has loss = 1.46115 in epoch 109\n",
      "Training example: 99 has loss = 1.46115 in epoch 110\n",
      "Training example: 1661 has loss = 1.46115 in epoch 111\n",
      "Training example: 1880 has loss = 1.46115 in epoch 112\n",
      "Training example: 1171 has loss = 1.46115 in epoch 113\n",
      "Training example: 819 has loss = 1.46115 in epoch 114\n",
      "Training example: 2062 has loss = 1.46115 in epoch 115\n",
      "Training example: 2836 has loss = 1.46115 in epoch 116\n",
      "Training example: 771 has loss = 1.46115 in epoch 117\n",
      "Training example: 2107 has loss = 1.46115 in epoch 118\n",
      "Training example: 2512 has loss = 1.46115 in epoch 119\n",
      "Training example: 2308 has loss = 1.46115 in epoch 120\n",
      "Training example: 1447 has loss = 1.46115 in epoch 121\n",
      "Training example: 2141 has loss = 1.46115 in epoch 122\n",
      "Training example: 75 has loss = 1.46115 in epoch 123\n",
      "Training example: 1280 has loss = 2.4611502 in epoch 124\n",
      "Training example: 1002 has loss = 1.46115 in epoch 125\n",
      "Training example: 1724 has loss = 1.46115 in epoch 126\n",
      "Training example: 735 has loss = 1.46115 in epoch 127\n",
      "Training example: 56 has loss = 1.46115 in epoch 128\n",
      "Training example: 2382 has loss = 1.46115 in epoch 129\n",
      "Training example: 1084 has loss = 1.46115 in epoch 130\n",
      "Training example: 1915 has loss = 1.46115 in epoch 131\n",
      "Training example: 1107 has loss = 1.46115 in epoch 132\n",
      "Training example: 1257 has loss = 1.46115 in epoch 133\n",
      "Training example: 1237 has loss = 1.46115 in epoch 134\n",
      "Training example: 475 has loss = 1.46115 in epoch 135\n",
      "Training example: 2115 has loss = 1.46115 in epoch 136\n",
      "Training example: 2947 has loss = 1.46115 in epoch 137\n",
      "Training example: 2597 has loss = 1.46115 in epoch 138\n",
      "Training example: 2863 has loss = 1.46115 in epoch 139\n",
      "Training example: 1276 has loss = 1.46115 in epoch 140\n",
      "Training example: 2431 has loss = 1.46115 in epoch 141\n",
      "Training example: 339 has loss = 1.46115 in epoch 142\n",
      "Training example: 687 has loss = 1.46115 in epoch 143\n",
      "Training example: 1344 has loss = 1.46115 in epoch 144\n",
      "Training example: 2332 has loss = 1.46115 in epoch 145\n",
      "Training example: 2167 has loss = 1.46115 in epoch 146\n",
      "Training example: 1284 has loss = 1.46115 in epoch 147\n",
      "Training example: 1401 has loss = 2.4611502 in epoch 148\n",
      "Training example: 1494 has loss = 1.46115 in epoch 149\n",
      "Training example: 526 has loss = 1.46115 in epoch 150\n",
      "Training example: 2963 has loss = 1.46115 in epoch 151\n",
      "Training example: 1254 has loss = 1.46115 in epoch 152\n",
      "Training example: 1917 has loss = 1.46115 in epoch 153\n",
      "Training example: 49 has loss = 1.46115 in epoch 154\n",
      "Training example: 2553 has loss = 1.46115 in epoch 155\n",
      "Training example: 320 has loss = 1.46115 in epoch 156\n",
      "Training example: 1036 has loss = 1.46115 in epoch 157\n",
      "Training example: 328 has loss = 1.46115 in epoch 158\n",
      "Training example: 2436 has loss = 1.46115 in epoch 159\n",
      "Training example: 2778 has loss = 1.46115 in epoch 160\n",
      "Training example: 400 has loss = 1.46115 in epoch 161\n",
      "Training example: 692 has loss = 1.46115 in epoch 162\n",
      "Training example: 2724 has loss = 1.46115 in epoch 163\n",
      "Training example: 24 has loss = 2.4611502 in epoch 164\n",
      "Training example: 1762 has loss = 1.46115 in epoch 165\n",
      "Training example: 2596 has loss = 1.46115 in epoch 166\n",
      "Training example: 571 has loss = 1.46115 in epoch 167\n",
      "Training example: 2767 has loss = 1.46115 in epoch 168\n",
      "Training example: 23 has loss = 1.46115 in epoch 169\n",
      "Training example: 1322 has loss = 1.46115 in epoch 170\n",
      "Training example: 2385 has loss = 1.46115 in epoch 171\n",
      "Training example: 2712 has loss = 1.46115 in epoch 172\n",
      "Training example: 1055 has loss = 1.46115 in epoch 173\n",
      "Training example: 2382 has loss = 1.46115 in epoch 174\n",
      "Training example: 1477 has loss = 1.46115 in epoch 175\n",
      "Training example: 930 has loss = 1.46115 in epoch 176\n",
      "Training example: 2128 has loss = 1.46115 in epoch 177\n",
      "Training example: 1532 has loss = 1.46115 in epoch 178\n",
      "Training example: 283 has loss = 1.46115 in epoch 179\n",
      "Training example: 910 has loss = 1.46115 in epoch 180\n",
      "Training example: 193 has loss = 1.46115 in epoch 181\n",
      "Training example: 1096 has loss = 1.46115 in epoch 182\n",
      "Training example: 47 has loss = 1.46115 in epoch 183\n",
      "Training example: 1506 has loss = 1.46115 in epoch 184\n",
      "Training example: 669 has loss = 2.4611502 in epoch 185\n",
      "Training example: 312 has loss = 1.46115 in epoch 186\n",
      "Training example: 675 has loss = 1.46115 in epoch 187\n",
      "Training example: 2912 has loss = 2.4611502 in epoch 188\n",
      "Training example: 2164 has loss = 1.46115 in epoch 189\n",
      "Training example: 2571 has loss = 1.46115 in epoch 190\n",
      "Training example: 2779 has loss = 1.46115 in epoch 191\n",
      "Training example: 1831 has loss = 1.46115 in epoch 192\n",
      "Training example: 2870 has loss = 1.46115 in epoch 193\n",
      "Training example: 2168 has loss = 1.46115 in epoch 194\n",
      "Training example: 2891 has loss = 1.46115 in epoch 195\n",
      "Training example: 2242 has loss = 1.46115 in epoch 196\n",
      "Training example: 1321 has loss = 1.46115 in epoch 197\n",
      "Training example: 338 has loss = 2.4611502 in epoch 198\n",
      "Training example: 61 has loss = 1.46115 in epoch 199\n",
      "Training example: 2961 has loss = 1.46115 in epoch 200\n",
      "Training example: 941 has loss = 1.46115 in epoch 201\n",
      "Training example: 1719 has loss = 1.46115 in epoch 202\n",
      "Training example: 545 has loss = 1.46115 in epoch 203\n",
      "Training example: 464 has loss = 1.46115 in epoch 204\n",
      "Training example: 2549 has loss = 1.46115 in epoch 205\n",
      "Training example: 985 has loss = 1.46115 in epoch 206\n",
      "Training example: 2129 has loss = 1.46115 in epoch 207\n",
      "Training example: 290 has loss = 1.46115 in epoch 208\n",
      "Training example: 1505 has loss = 1.46115 in epoch 209\n",
      "Training example: 2060 has loss = 1.46115 in epoch 210\n",
      "Training example: 1709 has loss = 1.46115 in epoch 211\n",
      "Training example: 2526 has loss = 1.46115 in epoch 212\n",
      "Training example: 2069 has loss = 1.46115 in epoch 213\n",
      "Training example: 2720 has loss = 2.4611502 in epoch 214\n",
      "Training example: 2643 has loss = 2.4611502 in epoch 215\n",
      "Training example: 1626 has loss = 2.4611502 in epoch 216\n",
      "Training example: 2699 has loss = 1.46115 in epoch 217\n",
      "Training example: 2731 has loss = 1.46115 in epoch 218\n",
      "Training example: 2598 has loss = 1.46115 in epoch 219\n",
      "Training example: 1246 has loss = 1.46115 in epoch 220\n",
      "Training example: 247 has loss = 1.46115 in epoch 221\n",
      "Training example: 1040 has loss = 1.46115 in epoch 222\n",
      "Training example: 1076 has loss = 1.46115 in epoch 223\n",
      "Training example: 590 has loss = 1.46115 in epoch 224\n",
      "Training example: 2266 has loss = 1.46115 in epoch 225\n",
      "Training example: 1513 has loss = 1.46115 in epoch 226\n",
      "Training example: 745 has loss = 1.46115 in epoch 227\n",
      "Training example: 1583 has loss = 1.46115 in epoch 228\n",
      "Training example: 125 has loss = 1.46115 in epoch 229\n",
      "Training example: 1211 has loss = 1.46115 in epoch 230\n",
      "Training example: 857 has loss = 1.46115 in epoch 231\n",
      "Training example: 1218 has loss = 1.46115 in epoch 232\n",
      "Training example: 1927 has loss = 1.46115 in epoch 233\n",
      "Training example: 2799 has loss = 1.46115 in epoch 234\n",
      "Training example: 1311 has loss = 1.46115 in epoch 235\n",
      "Training example: 1957 has loss = 1.46115 in epoch 236\n",
      "Training example: 207 has loss = 1.46115 in epoch 237\n",
      "Training example: 82 has loss = 1.46115 in epoch 238\n",
      "Training example: 1480 has loss = 1.46115 in epoch 239\n",
      "Training example: 279 has loss = 1.46115 in epoch 240\n",
      "Training example: 371 has loss = 1.46115 in epoch 241\n",
      "Training example: 1232 has loss = 1.46115 in epoch 242\n",
      "Training example: 540 has loss = 1.46115 in epoch 243\n",
      "Training example: 648 has loss = 1.46115 in epoch 244\n",
      "Training example: 955 has loss = 1.46115 in epoch 245\n",
      "Training example: 279 has loss = 1.46115 in epoch 246\n",
      "Training example: 55 has loss = 1.46115 in epoch 247\n",
      "Training example: 1542 has loss = 1.46115 in epoch 248\n",
      "Training example: 1053 has loss = 1.46115 in epoch 249\n",
      "Training example: 1361 has loss = 1.46115 in epoch 250\n"
     ]
    }
   ],
   "source": [
    "if (should_we_train == true)\n",
    "    number_of_epochs = 250;\n",
    "    for i = 1:number_of_epochs\n",
    "        \n",
    "        # train the model -\n",
    "        Flux.train!(model, training_image_dataset, opt_state) do m, x, y\n",
    "            loss(m(x), y)\n",
    "        end\n",
    "    \n",
    "        # output some stuff -\n",
    "        ridx = rand(1:number_of_training_examples);\n",
    "        test_x, test_y = training_image_dataset[ridx][1], training_image_dataset[ridx][2];\n",
    "        l = loss(model(test_x), test_y);\n",
    "        println(\"Training example: $(ridx) has loss = $(l) in epoch $(i)\");\n",
    "    \n",
    "        # save the state of the model, in case something happens. We can reload from this state\n",
    "        jldsave(\"tmp-model-training-checkpoint.jld2\", model_state = Flux.state(model))    \n",
    "    end\n",
    "else\n",
    "    model_state = JLD2.load(\"model-state-T3000-P500-E250-N512.jld2\", \"model_state\");\n",
    "    Flux.loadmodel!(model, model_state);\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae6d572-aa6b-4dda-88ea-c52d2a3bc495",
   "metadata": {},
   "source": [
    "## How well does the model predict unseen versus observed images?\n",
    "One of the challenges with [Artifical Neural Networks (ANNs)](https://en.wikipedia.org/wiki/Neural_network_(machine_learning)) is the lack of generalizability, i.e., they _may not_ perform well on data the model has not seen. Let's explore this question:\n",
    "* To begin with, let's compute the fraction of the `training data` that is not correctly classified. This will help us understand how many of the `N` training samples we get correct and how many we get wrong. We expect to be _mostly correct_ on this data.\n",
    "* Next, we'll do the same thing but with the `test data,` i.e., data the model has never seen. We expect the correct prediction fraction to be less than the equivalent training value on the `test data`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98398ccb-1ec6-40c7-9586-f3ad475011f1",
   "metadata": {},
   "source": [
    "### Compute the correct prediction fraction for the `training` and `test` datasets\n",
    "Process each image in the `testing_image_dataset` dataset. Pass the pixel data from the image into the `model` instance, compute the predicted label `ŷ,` and compare the predicted and actual labels. If they argree, we update the `S` variable (a running count of the number of correct predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c67a99f9-5efe-48de-a8f8-d9962dff24a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct prediction % on the training data: 89.93333333333334%\n"
     ]
    }
   ],
   "source": [
    "S_training = 0;\n",
    "for i ∈ eachindex(training_image_dataset)\n",
    "    \n",
    "    x = training_image_dataset[i][1];\n",
    "    y = training_image_dataset[i][2];\n",
    "    ŷ = model(x) |> z-> argmax(z) |> z-> number_digit_array[z] |> z-> onehot(z,number_digit_array)\n",
    "    y == ŷ ? S_training +=1 : nothing\n",
    "end\n",
    "correct_prediction_training = (S_training/length(training_image_dataset))*100;\n",
    "println(\"Correct prediction % on the training data: $(correct_prediction_training)%\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a021a683-8441-469d-8981-0c48800cf92b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct prediction on the test data: 89.64%\n"
     ]
    }
   ],
   "source": [
    "S_testing = 0;\n",
    "for i ∈ eachindex(testing_image_dataset)\n",
    "    \n",
    "    x = testing_image_dataset[i][1];\n",
    "    y = testing_image_dataset[i][2];\n",
    "    ŷ = model(x) |> z-> argmax(z) |> z-> number_digit_array[z] |> z-> onehot(z, number_digit_array)\n",
    "    y == ŷ ? S_testing+=1 : nothing\n",
    "end\n",
    "correct_prediction_test = (S_testing/length(testing_image_dataset))*100;\n",
    "println(\"Correct prediction on the test data: $(correct_prediction_test)%\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f277f5-eece-4075-9a84-6b56550475de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.3",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
